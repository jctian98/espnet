# Model
transformer: huggingface
transformer_conf:
    hf_model_tag: allenai/OLMo-2-1124-7B
    activation_checkpointing: true
    n_ctx: 16384
    
corelm: ar_delay
corelm_conf:
    share_emb: false

# loss
modality_weights: {
    "g2p": 1.0,
    "ssl": 0.5,
    "codec": 0.0625,
    "text_bpe": 1.0,
    "image": 1.0,
    "special_token": 10.0,
}
loss_region: target

# General Dataloader configuration
batch_type: numel
batch_bins: 16000
max_epoch: 10
num_workers: 6
drop_last_iter: true
log_interval: 100

# Data preprocessing
codec_token_per_frame: 9
codec_token_in_use: 9
speaker_prompt_length: 500

# specaug
asr_apply_time_mask: true
asr_time_mask_config:
    mask_width_ratio_range:
    - 0.
    - 0.05
    num_mask: 10

# deepspeed
use_deepspeed: true
deepspeed_config: conf/deepspeed_ddp_zero2_olmo.json
deepspeed_step_sync: false

# Pre-Trained Model
init_param: 
  - exp/opuslm_7b_baseline/205epoch_revised.pth

seed: 999
