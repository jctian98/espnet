# Model
transformer: huggingface
transformer_conf:
    hf_model_tag: Qwen/Qwen2.5-1.5B
    activation_checkpointing: false
    attention_choice: flash_attention_2
    n_ctx: 16000

corelm: ar_delay
corelm_conf:
    share_emb: false

text_encoder: hf
text_encoder_conf:
    hf_tag: google/flan-t5-xl
    connector_choice: linear

freeze_param:
  - corelm.continuous_encoders.text_encoder.model

# loss
modality_weights: {
    "codec": 0.125,
    "text_bpe": 1.0,
    "special_token": 10.0,
}
loss_region: target

# General Dataloader configuration
batch_type: numel
batch_bins: 12000
num_iters_per_epoch: 15000
# epoch_this_run: 1
max_epoch: 50
num_workers: 4
drop_last_iter: true
log_interval: 100

# Data preprocessing
codec_token_per_frame: 8
codec_token_in_use: 8
speaker_prompt_length: 500

# deepspeed
use_deepspeed: true
deepspeed_config: conf/deepspeed_ddp_zero1.json
deepspeed_step_sync: false
